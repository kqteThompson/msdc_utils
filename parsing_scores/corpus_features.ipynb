{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare corpora\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rels_no = {'COM':0, 'CONTR':1, 'CORR':2, 'QAP':3, 'ACK':4,'ELAB':5,\n",
    "                 'CLARIFQ':6, 'COND':7, 'CONTIN':8, 'RES':9, 'EXPL':10, 'QELAB':11,\n",
    "                 'ALT':12, 'NARR':13, 'CONFQ':14, 'SEQ':15, 'NULL':16}\n",
    "\n",
    "# reverse_relations = {0:'COM', 1: 'CONTR', 2 :'CORR', 3: 'QAP', 4: 'ACK', 5: 'ELAB',\n",
    "#             6: 'CLARIFQ', 7: 'COND', 8: 'CONTIN', 9: 'RES', 10: 'EXPL',\n",
    "#             11: 'QELAB', 12: 'ALT', 13: 'NARR', 14: 'CONFQ', 15: 'SEQ', 16: 'NULL'}\n",
    "\n",
    "reverse_relations = {'Comment':0, 'Contrast':1, 'Correction':2, 'Question-answer_pair':3, 'Acknowledgement':4,'Elaboration':5,\n",
    "                 'Clarification_question':6, 'Conditional':7, 'Continuation':8, 'Result':9, 'Explanation':10, 'Q-Elab':11,\n",
    "                 'Alternation':12, 'Narration':13, 'Confirmation_question':14, 'Sequence':15, 'Break':16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rels_str = {'Comment':'COM', 'Contrast':'CONTR', 'Correction':'CORR', 'Question-answer_pair':'QAP', 'Acknowledgement':'ACK','Elaboration':'ELAB',\n",
    "                 'Clarification_question':'CLARIFQ', 'Conditional':'COND', 'Continuation':'CONTIN', 'Result':'RES', 'Explanation':'EXPL', 'Q-Elab':'QELAB',\n",
    "                 'Alternation':'ALT', 'Narration':'NARR', 'Background':'BACK', 'Parallel':'PAR', 'Sequence':'SEQ', 'Question_answer_pair':'QAP',  \n",
    "                 'Q_Elab':'QELAB', 'Confirmation_question' : 'CONFQ'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAC LINGUISTIC TEST\n",
    "\n",
    "game pilot02_fix9 has only one edu\n",
    "game s2-leagueM-game4_fix14 has only one edu\n",
    "\n",
    "s1-league3-game3_fix24: [2], --> 2-->COMMENT-->3\n",
    "s2-league4-game2_fix5: [4],  --> 4---> COMMENT --> 5\n",
    "s2-league4-game2_fix17: [7], --->7 --> QAP --> 8\n",
    "s2-league4-game2_fix23: [3]  ** NO RELATION\n",
    "\n",
    "\n",
    "MSDC \n",
    "defaultdict(list,\n",
    "            {'C70-B11-A7': [60],\n",
    "             'C105-B35-A39': [38],\n",
    "             'C101-B44-A36': [35],\n",
    "             'C86-B16-A26': [11],\n",
    "             'C70-B29-A26': [54],\n",
    "             'C111-B35-A44': [14],\n",
    "             'C52-B36-A38': [8]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STAC COUNTS real quick\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "home=%pwd\n",
    "newstac_path = '/home/kate/minecraft_utils/stac_linguistic/stac_linguistic_flat.json'\n",
    "stac_path = '/home/kate/minecraft_utils/llm_annotator/stac/stac_linguistic_corrected/train_data.json'\n",
    "# stac_path = '/home/kate/minecraft_utils/llm_annotator/stac/stac_squished_corrected/train_data.json'\n",
    "# molweni_path = '/home/kate/minecraft_utils/llm_annotator/molweni/molweni_clean_test50.json'\n",
    "# mc_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/DEV_32_bert.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stac_path, 'r') as j:\n",
    "    jfile = json.load(j)\n",
    "    data = jfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguistic test\n",
    "Counter({'s2-league4-game2': 45,\n",
    "         's1-league3-game3': 26,\n",
    "         'pilot02': 25,\n",
    "         's2-leagueM-game4': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = []\n",
    "for d in data:\n",
    "    games.append(d['id'].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnt = Counter(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = defaultdict(list) #ids and list of nodes that do not have incoming links\n",
    "num_orphans = 0\n",
    "headless = []\n",
    "one_edu = 0\n",
    "for d in data:\n",
    "    if len(d['edus']) > 1:\n",
    "        all_nodes = [r for r in range(0,len(d['edus']))]\n",
    "        all_nodes.pop(0)\n",
    "        # print(all_nodes)\n",
    "        all_heads = [t['x'] for t in d['relations']]\n",
    "        if 0 not in all_heads:\n",
    "            headless.append(d['id'])\n",
    "        all_targets = [t['y'] for t in d['relations']]\n",
    "        #print(all_targets)\n",
    "        for node in all_nodes:\n",
    "            if node not in all_targets:\n",
    "                # missing_links[d['id']].append(str(node) + ' : ' + d['edus'][node]['text'])\n",
    "                missing_links[d['id']].append(node)\n",
    "                num_orphans += 1\n",
    "    else:\n",
    "        one_edu += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_orphans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fully connected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(newstac_path, 'r') as j:\n",
    "    jfile = json.load(j)\n",
    "    data = jfile['dialogues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = []\n",
    "for d in data:\n",
    "    if 'pilot' in d['dialogue_id']:\n",
    "        games.append(d['dialogue_id'].split('_')[1])\n",
    "    elif 'practice' in d['dialogue_id']:\n",
    "        newid = d['dialogue_id'].split('_')[1:3]\n",
    "        n = '-'.join(newid)\n",
    "        games.append(n)\n",
    "    else:\n",
    "        newid = d['dialogue_id'].split('_')[1:4]\n",
    "        n = '-'.join(newid)\n",
    "        games.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnt = Counter(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "for it in fcnt.items():\n",
    "    comparisons.append((it[0], it[1], gcnt[it[0]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pilot01', 45, 45),\n",
       " ('pilot02', 24, 0),\n",
       " ('pilot03', 35, 36),\n",
       " ('pilot04', 30, 30),\n",
       " ('pilot14', 36, 39),\n",
       " ('pilot20', 25, 26),\n",
       " ('pilot21', 23, 23),\n",
       " ('s1-league1-game1', 30, 31),\n",
       " ('s1-league1-game2', 29, 30),\n",
       " ('s1-league1-game3', 43, 45),\n",
       " ('s1-league1-game4', 38, 39),\n",
       " ('s1-league1-game5', 31, 32),\n",
       " ('s1-league2-game1', 37, 37),\n",
       " ('s1-league2-game2', 25, 25),\n",
       " ('s1-league2-game3', 19, 19),\n",
       " ('s1-league2-game4', 20, 24),\n",
       " ('s1-league3-game1', 22, 22),\n",
       " ('s1-league3-game2', 14, 14),\n",
       " ('s1-league3-game3', 26, 0),\n",
       " ('s1-league3-game4', 7, 7),\n",
       " ('s1-league3-game5', 16, 16),\n",
       " ('s1-league3-game6', 13, 13),\n",
       " ('s1-league3-game7', 19, 20),\n",
       " ('s2-league1-game1', 30, 33),\n",
       " ('s2-league3-game1', 18, 18),\n",
       " ('s2-league3-game4', 12, 12),\n",
       " ('s2-league3-game5', 24, 24),\n",
       " ('s2-league4-game1', 38, 38),\n",
       " ('s2-league4-game2', 45, 0),\n",
       " ('s2-league4-game3', 34, 34),\n",
       " ('s2-league5-game0', 6, 6),\n",
       " ('s2-league5-game1', 15, 18),\n",
       " ('s2-league5-game2', 32, 35),\n",
       " ('s2-league5-game3', 29, 32),\n",
       " ('s2-league5-game4', 40, 42),\n",
       " ('s2-league5-game5', 24, 24),\n",
       " ('s2-league8-game1', 3, 3),\n",
       " ('s2-league8-game2', 2, 2),\n",
       " ('s2-leagueM-game2', 21, 21),\n",
       " ('s2-leagueM-game3', 28, 31),\n",
       " ('s2-leagueM-game4', 14, 0),\n",
       " ('s2-leagueM-game5', 19, 20),\n",
       " ('s2-practice2', 16, 32),\n",
       " ('s2-practice3', 22, 44),\n",
       " ('s2-practice4', 22, 44)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = defaultdict(list) #ids and list of nodes that do not have incoming links\n",
    "num_orphans = 0\n",
    "headless = []\n",
    "for d in data:\n",
    "    all_nodes = [r for r in range(0,len(d['edus']))]\n",
    "    all_nodes.pop(0)\n",
    "    # print(all_nodes)\n",
    "    all_heads = [t['x'] for t in d['relations']]\n",
    "    if 0 not in all_heads:\n",
    "        headless.append(d['dialogue_id'])\n",
    "    all_targets = [t['y'] for t in d['relations']]\n",
    "    #print(all_targets)\n",
    "    for node in all_nodes:\n",
    "        if node not in all_targets:\n",
    "            # missing_links[d['id']].append(str(node) + ' : ' + d['edus'][node]['text'])\n",
    "            missing_links[d['dialogue_id']].append(node)\n",
    "            num_orphans += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_orphans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backwards relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkwds = defaultdict(list)\n",
    "for d in data:\n",
    "    for rel in d['relations']:\n",
    "        if rel['y'] < rel['x']:\n",
    "            bkwds[map_rels_str[rel['type']]].append(rel['y'] - rel['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bkwds.keys():\n",
    "    print('{} rel : {}'.format(k, len(bkwds[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAC SQUISHED\n",
    "COND rel : 68\n",
    "COM rel : 57\n",
    "EXPL rel : 4\n",
    "BACK rel : 10\n",
    "ELAB rel : 3\n",
    "\n",
    "STAC LINGUISTIC\n",
    "COND rel : 67\n",
    "CONTIN rel : 1\n",
    "COM rel : 61\n",
    "BACK rel : 8\n",
    "ELAB rel : 3\n",
    "EXPL rel : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edus = 0\n",
    "eeus = 0\n",
    "for d in stac['dialogues']:\n",
    "    total_dus = len(d['edus'])\n",
    "    eeu_count = len([e for e in d['edus'] if e['speaker'] in ['UI', 'Server']])\n",
    "    eeus += eeu_count\n",
    "    edu_count = total_dus - eeu_count\n",
    "    edus += edu_count\n",
    "\n",
    "print(\"edus: \", edus)\n",
    "print(\"eeus: \", eeus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdus = 0\n",
    "for d in stac['dialogues']:\n",
    "    cdus += len(d['cdus'])\n",
    "\n",
    "print(\"cdus: \", cdus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = 0\n",
    "for d in stac['dialogues']:\n",
    "    rels += len(d['relations'])\n",
    "\n",
    "print(\"rels: \", rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdu = 0\n",
    "for d in stac['dialogues']:\n",
    "    \n",
    "    rels_count = Counter([g['target'] for g in d['relations']])\n",
    "    mps = [r[0] for r in rels_count.items() if r[1] > 1] \n",
    "    mpdu += len(mps)\n",
    "\n",
    "print(\"mpdus: \", mpdu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #these are the STAC relations\n",
    "# map_relations = {'COM': 0, 'CONT': 1, 'CORR': 2, 'QAP': 3, 'PAR': 4, 'ACK': 5,\n",
    "#             'ELAB': 6, 'CLARIFQ': 7, 'COND': 8, 'CONTIN': 9, 'RES': 10, 'EXPL': 11,\n",
    "#             'QELAB': 12, 'ALT': 13, 'NARR': 14, 'BACK': 15, 'NULL': 16, 'SEQ' : 17}\n",
    "\n",
    "# reverse_relations = {0:'COM', 1: 'CONT', 2 :'CORR', 3: 'QAP', 4: 'PAR', 5: 'ACK',\n",
    "#             6: 'ELAB', 7: 'CLARIFQ', 8: 'COND', 9: 'CONT', 10: 'RES', 11: 'EXPL',\n",
    "#             12: 'QELAB', 13: 'ALT', 14: 'NARR', 15: 'BACK', 16: 'NULL', 17: 'SEQ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['COM','CONTR','CORR','QAP','ACK','ELAB','CLARIFQ','COND','CONTIN',\n",
    "#               'RES','EXPL','QELAB','ALT','NARR','CONFQ','SEQ','NULL']\n",
    "\n",
    "# labels = ['COM', 'CONT', 'CORR', 'QAP', 'PAR', 'ACK',\n",
    "#             'ELAB', 'CLARIFQ', 'COND', 'CONT', 'RES', 'EXPL',\n",
    "#             'QELAB', 'ALT', 'NARR', 'BACK', 'NULL', 'SEQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTLine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narration counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "picklepath = home + '/msdc_bert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(picklepath + 'full_scores_multi_d10_NEWTEST.pkl', 'rb') as f:\n",
    "    test_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_distances = []\n",
    "tp_distances = []\n",
    "fp_distances = []\n",
    "fn_distances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test_multi:\n",
    "    if t[3] == 13 and t[4] == 13:\n",
    "        tp_distances.append(t[2] - t[1])\n",
    "    elif t[3] == 13 and t[4] != 13:\n",
    "        fn_distances.append(t[2] - t[1])\n",
    "    elif t[3] != 13 and t[4] == 13:\n",
    "        fp_distances.append(t[2] - t[1])\n",
    "    \n",
    "    if t[3] == 13:\n",
    "        gold_distances.append(t[2] - t[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_counts = Counter(tp_distances)\n",
    "fp_counts = Counter(fp_distances)\n",
    "fn_counts = Counter(fn_distances)\n",
    "gold_counts = Counter(gold_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d for d in range(1,16)]\n",
    "head = ['gold', 'tp', 'fn', 'fp', 'F1']\n",
    "data = [] #a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in labels:\n",
    "    row = []\n",
    "    for count_list in [gold_counts, tp_counts, fn_counts, fp_counts]:\n",
    "        num = [c[1] for c in count_list.items() if c[0] == d]\n",
    "        if len(num) > 0:\n",
    "            row.append(num[0])\n",
    "        else:\n",
    "            row.append(0)\n",
    "        #calculate distance F1\n",
    "    if row[1] != 0:\n",
    "        microf1 = round(row[1]/(row[1] + 0.5*(row[3] + row[2])), 2)\n",
    "    else:\n",
    "        microf1 = 0.0\n",
    "    row.append(microf1)\n",
    "\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bertline Narration predictions')\n",
    "print('                                         ')\n",
    "print(pandas.DataFrame(data, labels, head))\n",
    "print('                                          ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END Narration counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "picklepath = home + '/msdc_bert/'\n",
    "# picklepath = home + '/stac_bert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(picklepath + 'scores_linear_d10_newtest.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'full_scores_multi_d10_NEWTEST.pkl', 'rb') as f:\n",
    "    test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'comparison_multi_stac.pkl', 'rb') as f:\n",
    "#     test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'linear_preds_stac.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'comparison_multi_stac_connected.pkl', 'rb') as f:\n",
    "#     test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'linear_preds_stac_connect.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that every edu is attached somewhere \n",
    "test_multi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_preds = defaultdict(list)\n",
    "graph_dict_gold = defaultdict(list)\n",
    "for t in test_multi:\n",
    "    if t[4] != 16:\n",
    "        graph_dict_preds[t[0]].append(t[2])\n",
    "    if t[3] != 16:\n",
    "        graph_dict_gold[t[0]].append(t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = 0\n",
    "for r in range(100):\n",
    "    p = graph_dict_preds[r]\n",
    "    g = graph_dict_gold[r]\n",
    "    #assert len(p) <= len(g), 'at index {}'.format(r) #make sure the preds are contained in the gold\n",
    "    missing = len(set(g) - set(p))\n",
    "    missing_links += missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_gold[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lin[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([t for t in test_lin if t[5] == 1 or t[3] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_attach = [t[3] for t in test_lin]\n",
    "attach_preds = [t[5] for t in test_lin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf = precision_recall_fscore_support(true_attach, attach_preds, average='binary')\n",
    "print(\"Attachment F1:\", prf[2])\n",
    "print(\"Attachment Average Precision:\", prf[0])\n",
    "print(\"Attachment Average Recall:\", prf[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [reverse_relations[t[3]] for t in test_multi]\n",
    "labels_pred = [reverse_relations[t[4]] for t in test_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_labels, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(home + \"/bert_matrix.txt\",\"w\")\n",
    "print(classification_report(true_labels, labels_pred), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[3] for t in test_multi]\n",
    "pred_list = [t[4] for t in test_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = classification_report(gold_list,pred_list,target_names=labels,output_dict=True)\n",
    "prec = 0\n",
    "rec = 0\n",
    "f1 = 0 \n",
    "count = 0\n",
    "\n",
    "for label in labels:\n",
    "    if label!=\"NULL\":\n",
    "        prec+=d[label][\"precision\"]*d[label][\"support\"]\n",
    "        rec+=d[label][\"recall\"]*d[label][\"support\"]\n",
    "        f1+=d[label][\"f1-score\"]*d[label][\"support\"]\n",
    "        count+=d[label][\"support\"]\n",
    "        # checking that support is same as the number of ground truth instance for the label\n",
    "        #assert d[label][\"support\"] == Counter(g_label_l)[label]\n",
    "        \n",
    "\n",
    "\n",
    "print(\"Weighted Average Precision:\", prec/count)\n",
    "print(\"Weighted Average Recall:\", rec/count)\n",
    "print(\"Weighted Average F1 score:\", f1/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#change to numbers \n",
    "'COM':0, \n",
    "'CONTR':1, \n",
    "'CORR':2, \n",
    "'QAP':3, \n",
    "'ACK':4,\n",
    "'ELAB':5,\n",
    "'CLARIFQ':6, \n",
    "'COND':7, \n",
    "'CONTIN':8, \n",
    "'RES':9, \n",
    "'EXPL':10, \n",
    "'QELAB':11,\n",
    "'ALT':12, \n",
    "'NARR':13, \n",
    "'CONFQ':14, 'SEQ':15, 'NULL':16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [map_rels_no[t] for t in corr_]\n",
    "predicted = [map_rels_no[t] for t in pred_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(correct,predicted)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(picklepath + 'tp_dict.pkl', 'rb') as f:\n",
    "    tp_dict = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'fn_dict.pkl', 'rb') as f:\n",
    "    fn_dict = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'fp_dict.pkl', 'rb') as f:\n",
    "    fp_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(tp_dict.keys()):\n",
    "    print(key, len(tp_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tp_dict[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tp_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(fn_dict.keys()):\n",
    "    print(key, len(fn_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiparent edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "gold_data_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/TEST_101_bert.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gold_data_path, 'r') as j:\n",
    "    jfile = json.load(j)\n",
    "    gold = jfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_count = [] #these are the ones that have double endpoints in common\n",
    "pred_count = []\n",
    "type_compare = [] #[i, x, y, gold_rel, pred_re] AND source compare\n",
    "\n",
    "for i, game in enumerate(gold[:1]):\n",
    "    rels_count = Counter([g['y'] for g in game['relations']])\n",
    "    count = [r[0] for r in rels_count.items() if r[1] > 1]\n",
    "    #print(count)\n",
    "    mp_list = []\n",
    "    for r in gold[i]['relations']:\n",
    "        if r['y'] in count:\n",
    "            mp_list.append([i, r['x'], r['y'], reverse_relations[r['type']]])\n",
    "    # for m in mp_list:\n",
    "    #     print(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mpedus_gold = 0\n",
    "total_mpedus_pred = 0\n",
    "total_overlap_target = []\n",
    "total_target_accuracy = []\n",
    "\n",
    "for i, game in enumerate(gold):\n",
    "    #get predictions for this game -- all predicted endpoints\n",
    "    preds = [e for e in test_multi if e[0] == i and e[4] != 16]\n",
    "    rels_count = Counter([g['y'] for g in game['relations']])\n",
    "    gold_count = [r[0] for r in rels_count.items() if r[1] > 1]\n",
    "    total_mpedus_gold += len(gold_count)\n",
    "    preds_counter = Counter([p[2] for p in preds])\n",
    "    preds_count = [r[0] for r in preds_counter.items() if r[1] > 1] \n",
    "    total_mpedus_pred += len(preds_count)\n",
    "    #get matching targets in pred\n",
    "    # print(\"gold multi: \", len(gold_count))\n",
    "    # print(\"pred multi: \", len(preds_count))\n",
    "    overlap = [m for m in preds_count if m in gold_count]\n",
    "    # print('overlap: ', len(overlap))\n",
    "    # print(overlap)\n",
    "    # print('-------------------------------')\n",
    "    total_overlap_target.append(len(overlap)/len(gold_count)) #save total overlap\n",
    "    #total_target_accuracy = []\n",
    "    #now for each target in overlap, see if the source is correct\n",
    "    for target in overlap:\n",
    "        gold_sources = [gr['x'] for gr in gold[i]['relations'] if gr['y'] == target]\n",
    "        preds_sources = [pr[1] for pr in preds if pr[2] == target]\n",
    "        accuracy = len([n for n in preds_sources if n in gold_sources])/len(gold_sources)\n",
    "        total_target_accuracy.append(accuracy)\n",
    "\n",
    "        # print('target: {} '.format(target))\n",
    "        # print('gold sources: {}'.format(gold_sources))\n",
    "        # print('pred sources: {} '.format(preds_sources))\n",
    "        # print('accuracy : {}'.format(accuracy))\n",
    "        # print('---------------------------------')\n",
    "\n",
    "\n",
    "#now for each target in overlap, see if target is correct \n",
    "print('total mpdus in gold: {}'.format(total_mpedus_gold))\n",
    "print('total mpdus predicted: {}'.format(total_mpedus_pred))\n",
    "print('mean target accuracy: {}'.format(np.mean(total_overlap_target)))\n",
    "print('mean source accuracy: {}'.format(np.mean(total_target_accuracy)))\n",
    "\n",
    "##so we got about 45 percent of the multiparent edus correct, and of those, we got 87 percent of the sources correct \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
