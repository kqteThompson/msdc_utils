{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rels_no = {'COM':0, 'CONTR':1, 'CORR':2, 'QAP':3, 'ACK':4,'ELAB':5,\n",
    "                 'CLARIFQ':6, 'COND':7, 'CONTIN':8, 'RES':9, 'EXPL':10, 'QELAB':11,\n",
    "                 'ALT':12, 'NARR':13, 'CONFQ':14, 'SEQ':15, 'NULL':16}\n",
    "\n",
    "reverse_relations = {0:'COM', 1: 'CONTR', 2 :'CORR', 3: 'QAP', 4: 'ACK', 5: 'ELAB',\n",
    "            6: 'CLARIFQ', 7: 'COND', 8: 'CONTIN', 9: 'RES', 10: 'EXPL',\n",
    "            11: 'QELAB', 12: 'ALT', 13: 'NARR', 14: 'CONFQ', 15: 'SEQ', 16: 'NULL'}\n",
    "\n",
    "# reverse_relations = {'Comment':0, 'Contrast':1, 'Correction':2, 'Question-answer_pair':3, 'Acknowledgement':4,'Elaboration':5,\n",
    "#                  'Clarification_question':6, 'Conditional':7, 'Continuation':8, 'Result':9, 'Explanation':10, 'Q-Elab':11,\n",
    "#                  'Alternation':12, 'Narration':13, 'Confirmation_question':14, 'Sequence':15, 'Break':16}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rels_str = {'Comment':'COM', 'Contrast':'CONTR', 'Correction':'CORR', 'Question-answer_pair':'QAP', 'Acknowledgement':'ACK','Elaboration':'ELAB',\n",
    "                 'Clarification_question':'CLARIFQ', 'Conditional':'COND', 'Continuation':'CONTIN', 'Result':'RES', 'Explanation':'EXPL', 'Q-Elab':'QELAB',\n",
    "                 'Alternation':'ALT', 'Narration':'NARR', 'Background':'BACK', 'Parallel':'PAR', 'Sequence':'SEQ', 'Question_answer_pair':'QAP',  \n",
    "                 'Q_Elab':'QELAB', 'Confirmation_question' : 'CONFQ'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAC counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STAC COUNTS real quick\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import jsonlines\n",
    "import pickle\n",
    "home=%pwd\n",
    "mc_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/TEST_101_bert.json'\n",
    "local_path = '/home/kate/minecraft_utils/llm_annotator/local_model_flat_test.jsonl'\n",
    "local_flat_path = '/home/kate/minecraft_utils/llm_annotator/local_model_flat_train.jsonl'\n",
    "mc_check = '/home/kate/LREC/data/TEST_101_bert.json'\n",
    "llama_parse = '/home/kate/minecraft_utils/llm_annotator/parser_test_moves_15.jsonl'\n",
    "bert_data = '/home/kate/minecraft_utils/parsing_scores/msdc_bert/full_scores_multi_d10.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_data, 'rb') as f:\n",
    "    test_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test = []\n",
    "for t in test_multi:\n",
    "    bert_test.append(reverse_relations[t[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(bert_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(bert_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mc_check, 'r') as j:\n",
    "    jfile = json.load(j)\n",
    "    data = jfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check relation counts\n",
    "rels = []\n",
    "for d in data:\n",
    "    rels.extend([d['type'] for d in d['relations'] if d['x'] < d['y'] and abs(d['y'] - d['x']) <=10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local = []\n",
    "# with jsonlines.open(local_path) as reader:\n",
    "#     for i, obj in enumerate(reader):\n",
    "#         if i%2 == 0:\n",
    "#             s = obj['sample'].split('\\n')\n",
    "#             out = (i, obj['PS'], s[0], s[1])\n",
    "#             local.append(out)\n",
    "\n",
    "local = []\n",
    "with jsonlines.open(local_path) as reader:\n",
    "    for i, obj in enumerate(reader):\n",
    "        if obj['PS'] != 'NONE':\n",
    "            local.append(obj['PS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'QAP': 305,\n",
       "         'COM': 164,\n",
       "         'ACK': 147,\n",
       "         'CONTIN': 112,\n",
       "         'ELAB': 101,\n",
       "         'QELAB': 72,\n",
       "         'CONTR': 43,\n",
       "         'CLARIFQ': 33,\n",
       "         'EXPL': 30,\n",
       "         'RES': 28,\n",
       "         'CORR': 21,\n",
       "         'ALT': 19,\n",
       "         'PAR': 15,\n",
       "         'COND': 11,\n",
       "         'NARR': 9})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10878"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check RES\n",
    "loc_res = []\n",
    "for loc in local:\n",
    "    if loc[1] == 'RES':\n",
    "        edu_one_ind = loc[2].split('<')[0].strip()\n",
    "        edu_one_text = loc[2].split('>')[1].strip()\n",
    "        edu_two_ind = loc[3].split('<')[0].strip()\n",
    "        edu_two_text = loc[3].split('>')[1].strip()\n",
    "        loc_res.append((loc[0], edu_one_ind, edu_one_text, edu_two_ind, edu_two_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loc_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backwards relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkwds = defaultdict(list)\n",
    "for d in data:\n",
    "    for rel in d['relations']:\n",
    "        if rel['y'] < rel['x']:\n",
    "            bkwds[map_rels_str[rel['type']]].append(rel['y'] - rel['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bkwds.keys():\n",
    "    print('{} rel : {}'.format(k, len(bkwds[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAC SQUISHED\n",
    "COND rel : 68\n",
    "COM rel : 57\n",
    "EXPL rel : 4\n",
    "BACK rel : 10\n",
    "ELAB rel : 3\n",
    "\n",
    "STAC LINGUISTIC\n",
    "COND rel : 67\n",
    "CONTIN rel : 1\n",
    "COM rel : 61\n",
    "BACK rel : 8\n",
    "ELAB rel : 3\n",
    "EXPL rel : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edus = 0\n",
    "eeus = 0\n",
    "for d in stac['dialogues']:\n",
    "    total_dus = len(d['edus'])\n",
    "    eeu_count = len([e for e in d['edus'] if e['speaker'] in ['UI', 'Server']])\n",
    "    eeus += eeu_count\n",
    "    edu_count = total_dus - eeu_count\n",
    "    edus += edu_count\n",
    "\n",
    "print(\"edus: \", edus)\n",
    "print(\"eeus: \", eeus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdus = 0\n",
    "for d in stac['dialogues']:\n",
    "    cdus += len(d['cdus'])\n",
    "\n",
    "print(\"cdus: \", cdus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = 0\n",
    "for d in stac['dialogues']:\n",
    "    rels += len(d['relations'])\n",
    "\n",
    "print(\"rels: \", rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpdu = 0\n",
    "for d in stac['dialogues']:\n",
    "    \n",
    "    rels_count = Counter([g['target'] for g in d['relations']])\n",
    "    mps = [r[0] for r in rels_count.items() if r[1] > 1] \n",
    "    mpdu += len(mps)\n",
    "\n",
    "print(\"mpdus: \", mpdu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #these are the STAC relations\n",
    "# map_relations = {'COM': 0, 'CONT': 1, 'CORR': 2, 'QAP': 3, 'PAR': 4, 'ACK': 5,\n",
    "#             'ELAB': 6, 'CLARIFQ': 7, 'COND': 8, 'CONTIN': 9, 'RES': 10, 'EXPL': 11,\n",
    "#             'QELAB': 12, 'ALT': 13, 'NARR': 14, 'BACK': 15, 'NULL': 16, 'SEQ' : 17}\n",
    "\n",
    "# reverse_relations = {0:'COM', 1: 'CONT', 2 :'CORR', 3: 'QAP', 4: 'PAR', 5: 'ACK',\n",
    "#             6: 'ELAB', 7: 'CLARIFQ', 8: 'COND', 9: 'CONT', 10: 'RES', 11: 'EXPL',\n",
    "#             12: 'QELAB', 13: 'ALT', 14: 'NARR', 15: 'BACK', 16: 'NULL', 17: 'SEQ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['COM','CONTR','CORR','QAP','ACK','ELAB','CLARIFQ','COND','CONTIN',\n",
    "#               'RES','EXPL','QELAB','ALT','NARR','CONFQ','SEQ','NULL']\n",
    "\n",
    "# labels = ['COM', 'CONT', 'CORR', 'QAP', 'PAR', 'ACK',\n",
    "#             'ELAB', 'CLARIFQ', 'COND', 'CONT', 'RES', 'EXPL',\n",
    "#             'QELAB', 'ALT', 'NARR', 'BACK', 'NULL', 'SEQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTLine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narration counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "picklepath = home + '/msdc_bert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(picklepath + 'full_scores_multi_d10_NEWTEST.pkl', 'rb') as f:\n",
    "    test_multi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_distances = []\n",
    "tp_distances = []\n",
    "fp_distances = []\n",
    "fn_distances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test_multi:\n",
    "    if t[3] == 13 and t[4] == 13:\n",
    "        tp_distances.append(t[2] - t[1])\n",
    "    elif t[3] == 13 and t[4] != 13:\n",
    "        fn_distances.append(t[2] - t[1])\n",
    "    elif t[3] != 13 and t[4] == 13:\n",
    "        fp_distances.append(t[2] - t[1])\n",
    "    \n",
    "    if t[3] == 13:\n",
    "        gold_distances.append(t[2] - t[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_counts = Counter(tp_distances)\n",
    "fp_counts = Counter(fp_distances)\n",
    "fn_counts = Counter(fn_distances)\n",
    "gold_counts = Counter(gold_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d for d in range(1,16)]\n",
    "head = ['gold', 'tp', 'fn', 'fp', 'F1']\n",
    "data = [] #a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in labels:\n",
    "    row = []\n",
    "    for count_list in [gold_counts, tp_counts, fn_counts, fp_counts]:\n",
    "        num = [c[1] for c in count_list.items() if c[0] == d]\n",
    "        if len(num) > 0:\n",
    "            row.append(num[0])\n",
    "        else:\n",
    "            row.append(0)\n",
    "        #calculate distance F1\n",
    "    if row[1] != 0:\n",
    "        microf1 = round(row[1]/(row[1] + 0.5*(row[3] + row[2])), 2)\n",
    "    else:\n",
    "        microf1 = 0.0\n",
    "    row.append(microf1)\n",
    "\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bertline Narration predictions')\n",
    "print('                                         ')\n",
    "print(pandas.DataFrame(data, labels, head))\n",
    "print('                                          ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END Narration counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "picklepath = home + '/msdc_bert/'\n",
    "# picklepath = home + '/stac_bert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(picklepath + 'scores_linear_d10_newtest.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'full_scores_multi_d10_NEWTEST.pkl', 'rb') as f:\n",
    "    test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'comparison_multi_stac.pkl', 'rb') as f:\n",
    "#     test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'linear_preds_stac.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'comparison_multi_stac_connected.pkl', 'rb') as f:\n",
    "#     test_multi = pickle.load(f)\n",
    "\n",
    "# with open(picklepath + 'linear_preds_stac_connect.pkl', 'rb') as f:\n",
    "#     test_lin = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that every edu is attached somewhere \n",
    "test_multi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_preds = defaultdict(list)\n",
    "graph_dict_gold = defaultdict(list)\n",
    "for t in test_multi:\n",
    "    if t[4] != 16:\n",
    "        graph_dict_preds[t[0]].append(t[2])\n",
    "    if t[3] != 16:\n",
    "        graph_dict_gold[t[0]].append(t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = 0\n",
    "for r in range(100):\n",
    "    p = graph_dict_preds[r]\n",
    "    g = graph_dict_gold[r]\n",
    "    #assert len(p) <= len(g), 'at index {}'.format(r) #make sure the preds are contained in the gold\n",
    "    missing = len(set(g) - set(p))\n",
    "    missing_links += missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict_gold[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lin[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([t for t in test_lin if t[5] == 1 or t[3] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_attach = [t[3] for t in test_lin]\n",
    "attach_preds = [t[5] for t in test_lin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf = precision_recall_fscore_support(true_attach, attach_preds, average='binary')\n",
    "print(\"Attachment F1:\", prf[2])\n",
    "print(\"Attachment Average Precision:\", prf[0])\n",
    "print(\"Attachment Average Recall:\", prf[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [reverse_relations[t[3]] for t in test_multi]\n",
    "labels_pred = [reverse_relations[t[4]] for t in test_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_labels, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(home + \"/bert_matrix.txt\",\"w\")\n",
    "print(classification_report(true_labels, labels_pred), file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[3] for t in test_multi]\n",
    "pred_list = [t[4] for t in test_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = classification_report(gold_list,pred_list,target_names=labels,output_dict=True)\n",
    "prec = 0\n",
    "rec = 0\n",
    "f1 = 0 \n",
    "count = 0\n",
    "\n",
    "for label in labels:\n",
    "    if label!=\"NULL\":\n",
    "        prec+=d[label][\"precision\"]*d[label][\"support\"]\n",
    "        rec+=d[label][\"recall\"]*d[label][\"support\"]\n",
    "        f1+=d[label][\"f1-score\"]*d[label][\"support\"]\n",
    "        count+=d[label][\"support\"]\n",
    "        # checking that support is same as the number of ground truth instance for the label\n",
    "        #assert d[label][\"support\"] == Counter(g_label_l)[label]\n",
    "        \n",
    "\n",
    "\n",
    "print(\"Weighted Average Precision:\", prec/count)\n",
    "print(\"Weighted Average Recall:\", rec/count)\n",
    "print(\"Weighted Average F1 score:\", f1/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#change to numbers \n",
    "'COM':0, \n",
    "'CONTR':1, \n",
    "'CORR':2, \n",
    "'QAP':3, \n",
    "'ACK':4,\n",
    "'ELAB':5,\n",
    "'CLARIFQ':6, \n",
    "'COND':7, \n",
    "'CONTIN':8, \n",
    "'RES':9, \n",
    "'EXPL':10, \n",
    "'QELAB':11,\n",
    "'ALT':12, \n",
    "'NARR':13, \n",
    "'CONFQ':14, 'SEQ':15, 'NULL':16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [map_rels_no[t] for t in corr_]\n",
    "predicted = [map_rels_no[t] for t in pred_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(correct,predicted)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(picklepath + 'tp_dict.pkl', 'rb') as f:\n",
    "    tp_dict = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'fn_dict.pkl', 'rb') as f:\n",
    "    fn_dict = pickle.load(f)\n",
    "\n",
    "with open(picklepath + 'fp_dict.pkl', 'rb') as f:\n",
    "    fp_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(tp_dict.keys()):\n",
    "    print(key, len(tp_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tp_dict[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tp_dict[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(fn_dict.keys()):\n",
    "    print(key, len(fn_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiparent edus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "gold_data_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/TEST_101_bert.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gold_data_path, 'r') as j:\n",
    "    jfile = json.load(j)\n",
    "    gold = jfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_count = [] #these are the ones that have double endpoints in common\n",
    "pred_count = []\n",
    "type_compare = [] #[i, x, y, gold_rel, pred_re] AND source compare\n",
    "\n",
    "for i, game in enumerate(gold[:1]):\n",
    "    rels_count = Counter([g['y'] for g in game['relations']])\n",
    "    count = [r[0] for r in rels_count.items() if r[1] > 1]\n",
    "    #print(count)\n",
    "    mp_list = []\n",
    "    for r in gold[i]['relations']:\n",
    "        if r['y'] in count:\n",
    "            mp_list.append([i, r['x'], r['y'], reverse_relations[r['type']]])\n",
    "    # for m in mp_list:\n",
    "    #     print(m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mpedus_gold = 0\n",
    "total_mpedus_pred = 0\n",
    "total_overlap_target = []\n",
    "total_target_accuracy = []\n",
    "\n",
    "for i, game in enumerate(gold):\n",
    "    #get predictions for this game -- all predicted endpoints\n",
    "    preds = [e for e in test_multi if e[0] == i and e[4] != 16]\n",
    "    rels_count = Counter([g['y'] for g in game['relations']])\n",
    "    gold_count = [r[0] for r in rels_count.items() if r[1] > 1]\n",
    "    total_mpedus_gold += len(gold_count)\n",
    "    preds_counter = Counter([p[2] for p in preds])\n",
    "    preds_count = [r[0] for r in preds_counter.items() if r[1] > 1] \n",
    "    total_mpedus_pred += len(preds_count)\n",
    "    #get matching targets in pred\n",
    "    # print(\"gold multi: \", len(gold_count))\n",
    "    # print(\"pred multi: \", len(preds_count))\n",
    "    overlap = [m for m in preds_count if m in gold_count]\n",
    "    # print('overlap: ', len(overlap))\n",
    "    # print(overlap)\n",
    "    # print('-------------------------------')\n",
    "    total_overlap_target.append(len(overlap)/len(gold_count)) #save total overlap\n",
    "    #total_target_accuracy = []\n",
    "    #now for each target in overlap, see if the source is correct\n",
    "    for target in overlap:\n",
    "        gold_sources = [gr['x'] for gr in gold[i]['relations'] if gr['y'] == target]\n",
    "        preds_sources = [pr[1] for pr in preds if pr[2] == target]\n",
    "        accuracy = len([n for n in preds_sources if n in gold_sources])/len(gold_sources)\n",
    "        total_target_accuracy.append(accuracy)\n",
    "\n",
    "        # print('target: {} '.format(target))\n",
    "        # print('gold sources: {}'.format(gold_sources))\n",
    "        # print('pred sources: {} '.format(preds_sources))\n",
    "        # print('accuracy : {}'.format(accuracy))\n",
    "        # print('---------------------------------')\n",
    "\n",
    "\n",
    "#now for each target in overlap, see if target is correct \n",
    "print('total mpdus in gold: {}'.format(total_mpedus_gold))\n",
    "print('total mpdus predicted: {}'.format(total_mpedus_pred))\n",
    "print('mean target accuracy: {}'.format(np.mean(total_overlap_target)))\n",
    "print('mean source accuracy: {}'.format(np.mean(total_target_accuracy)))\n",
    "\n",
    "##so we got about 45 percent of the multiparent edus correct, and of those, we got 87 percent of the sources correct \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
