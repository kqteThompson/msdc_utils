{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b12bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_relations = {'Comment':0, 'Contrast':1, 'Correction':2, 'Question-answer_pair':3, 'Acknowledgement':4,'Elaboration':5,\n",
    "                 'Clarification_question':6, 'Conditional':7, 'Continuation':8, 'Result':9, 'Explanation':10, 'Q-Elab':11,\n",
    "                 'Alternation':12, 'Narration':13, 'Confirmation_question':14, 'Sequence':15, 'Null':16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944d2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_relations = {0:'Comment', 1:'Contrast', 2:'Correction', 3:'Question-answer_pair', 4:'Acknowledgement',5:'Elaboration',\n",
    "                 6:'Clarification_question', 7:'Conditional', 8:'Continuation', 9:'Result', 10:'Explanation', 11:'Q-Elab',\n",
    "                 12:'Alternation', 13:'Narration', 14:'Confirmation_question', 15:'Sequence', 16:'Null'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb2deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from collections import Counter, defaultdict \n",
    "from sklearn.metrics import precision_recall_fscore_support,classification_report, ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0cca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = %pwd\n",
    "# gold_test_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/TEST_101_bert.json'\n",
    "# llama_test_path = home + '/msdc_llama/test-output-generate-2p-format.pkl'\n",
    "gold_val_path = '/home/kate/minecraft_utils/llm_annotator/annotated_data/VAL_100_bert.json'\n",
    "llama_val_path = home + '/msdc_llama/val-output-generate-2p-format.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3cc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gold_val_path, 'r') as jf:\n",
    "    test_gold = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "964bbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(llama_val_path, 'rb') as f:\n",
    "    test_pred = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a502dd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6466, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred), len(test_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e004004",
   "metadata": {},
   "source": [
    "combine test pred and test gold info: <br>\n",
    "get speaker, global turn info from gold, and incoming result and continuation info from preds, <br>\n",
    "add them to the edu objects in the test gold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ad39e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 28, 32, 1, 9], [1, 26, 32, 1, 2], [1, 32, 33, 1, 14], [1, 33, 34, 1, 3]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[100:104]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333f258",
   "metadata": {},
   "source": [
    "##### step 1: add predicted relations to the gold data json <br>\n",
    "NB: test gold is a list of games, whereas test pred is a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa531db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, game in enumerate(test_gold):\n",
    "    preds = [[e[1], e[2], e[4]] for e in test_pred if e[0]==i]\n",
    "    pred_rels = []\n",
    "    for p in preds:\n",
    "        pred_rel = {}\n",
    "        pred_rel['x'] = p[0]\n",
    "        pred_rel['y'] = p[1]\n",
    "        pred_rel['type'] = reverse_relations[p[2]]\n",
    "        pred_rels.append(pred_rel)\n",
    "    game['predicted_relations'] = pred_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131a5a3",
   "metadata": {},
   "source": [
    "##### step 2 <br> adds info to edus that is necessary for more second pass (Narration) processing. \n",
    "<br> info: global turn, architect edu index in turn, 1/0 result incoming, 1/0 continuation, edu type <br> returns only narration relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a544415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nl(edu):\n",
    "    \"\"\"\n",
    "    if every word in alphanumeric\n",
    "    \"\"\"\n",
    "    nl = 1\n",
    "    words = edu.split(' ')\n",
    "    # print(words)\n",
    "    for word in [w for w in words if w != '']:\n",
    "        if not contains_number(word) or len(word)<5:\n",
    "            nl = 0\n",
    "            break\n",
    "    return nl\n",
    "\n",
    "def contains_number(string):\n",
    "    return any(char.isdigit() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77cef624",
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in test_gold:\n",
    "    rels = game['predicted_relations']\n",
    "    # rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    new_rels = []\n",
    "\n",
    "    #add turn index for arch and global turn info to all edus\n",
    "    ind_cnt = 0\n",
    "    global_cnt = 0\n",
    "    last_speaker = None\n",
    "    global_index = 0\n",
    "    for edu in edus:\n",
    "        edu['global_index'] = global_index\n",
    "        global_index += 1\n",
    "        speaker = edu['speaker']\n",
    "        if speaker == last_speaker:\n",
    "            edu['turn'] = global_cnt\n",
    "        else:\n",
    "            last_speaker = speaker\n",
    "            global_cnt += 1\n",
    "            edu['turn'] = global_cnt\n",
    "        if speaker == 'Architect':\n",
    "            edu['turn_ind'] = ind_cnt\n",
    "            ind_cnt += 1\n",
    "            edu['type'] = 0\n",
    "        elif speaker == 'Builder':\n",
    "            ind_cnt = 0\n",
    "            #also add type infoes\n",
    "            if is_nl(edu['text']):\n",
    "                edu['type'] = 1 \n",
    "            else:\n",
    "                edu['type'] = 0\n",
    "        #add field for incoming result information\n",
    "        edu['res'] = 0\n",
    "        edu['nar_start'] = 0\n",
    "\n",
    "    #add incoming Result information\n",
    "    for rel in rels:\n",
    "        ind = rel['y']\n",
    "        if rel['type'] == 'Result':\n",
    "            edus[ind]['res'] = 1\n",
    "        # if rel['type'] == 'Acknowledgement':\n",
    "        #     edus[ind]['ack'] = 1\n",
    "        #add first continuation!!!\n",
    "        if rel['x'] == 0 and rel['type'] == 'Continuation':\n",
    "            edus[ind]['nar_start'] = 1\n",
    "\n",
    "        #keep only specified type of relations\n",
    "#         if rel['type'] == 'Narration':\n",
    "#             new_rels.append(rel)         \n",
    "#     game['relations'] = new_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483f8df",
   "metadata": {},
   "source": [
    "##### step 4: run second pass\n",
    "<br> Overall, the idea is to go through each game extending a 'backbone' of Narrative arcs from Instruction 1 to Instruction 2, from Instruction 2 to Instruction 3, etc. with the last Narration from Instruction n-1 to Instrution n. \n",
    "<br>For each game, find the first EDU and check that is connected to the 0 EDU by a Continuation relation. This EDU will be the source EDU for the first Narrative arc. \n",
    "<br>For each subsequent EDU, update the state to reflect whether actions have taken taken place. The next EDU that occurs after actions have taken place, has an Architect as speaker, and is the target of a Result, becomes the target of the current Narrative arc, as well as the source of the next arc. \n",
    "<br> if there is no start found the game is ignored. \n",
    "<br> The output of this step is a list of games, each game is a list of narrative arc endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cae09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no start\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "no_start_games = []\n",
    "for i, game in enumerate(test_gold):\n",
    "        narration_guesses = []\n",
    "        actions_happening = 0\n",
    "        actions_happened = 0\n",
    "        narr = [0,0]\n",
    "        try:\n",
    "            start = [edu['global_index'] for edu in game['edus'] if edu['nar_start'] == 1][0]\n",
    "            narr[0] = start\n",
    "        except IndexError:\n",
    "            print('no start')\n",
    "            no_start_games.append(game['id'])\n",
    "            ##if no start can be found, then the source of the first narration is 0\n",
    "            pass   \n",
    "        for edu in game['edus'][start+1:]:\n",
    "            if edu['type'] == 1 and actions_happening == 0: #if builder is moving\n",
    "                actions_happening = 1\n",
    "            if edu['type'] == 0  and actions_happening == 1: #if someone is talking now\n",
    "                actions_happened = 1\n",
    "            if edu['speaker'] == 'Architect' and edu['res'] == 1 and actions_happened == 1:\n",
    "                narr[1] = edu['global_index']\n",
    "                #decide here how to return the information\n",
    "                full_tup = [i]\n",
    "                full_tup.extend(narr)\n",
    "                full_tup.extend([1,13])\n",
    "                narration_guesses.append(full_tup) #!!!return tups with game index\n",
    "                ##originally append a tuple\n",
    "                narr = [0,0]\n",
    "                narr[0] = edu['global_index']\n",
    "                actions_happening = 0\n",
    "                actions_happened = 1\n",
    "        predictions.append(narration_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95eede12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fea4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home + '/pickles/' + 'llamipa_second_pass.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609e15f",
   "metadata": {},
   "source": [
    "##### step 5: get F1 for second pass on Narration arcs only\n",
    "<br> This ignores intra-turn Narrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6fc57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format gold -- ignore intra turn narrations!!\n",
    "gold_narrs = []\n",
    "for game in test_gold:\n",
    "    narrs = []\n",
    "    rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        if rel['type'] == 'Narration':\n",
    "            if turn_dict[rel['x']] != turn_dict[rel['y']]:\n",
    "                # if rel['y'] - rel['x'] < 11: ### D == 10 or less\n",
    "                if rel['y'] - rel['x'] < 16: ### D == 15 or less\n",
    "                    narrs.append((rel['x'], rel['y']))\n",
    "    gold_narrs.append(narrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fc5e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now rearrange\n",
    "gold_narrations = []\n",
    "pred_narrations = []\n",
    "\n",
    "for i, game in enumerate(gold_narrs):\n",
    "    #preds = predictions[i]\n",
    "    # preds = [(p[1], p[2]) for p in predictions[i] if p[2]-p[1] < 11] ### D == 10 or less\n",
    "    preds = [(p[1], p[2]) for p in predictions[i] if p[2]-p[1] < 16] ### D == 15 or less\n",
    "    for rel in game:\n",
    "        if rel in preds: #TP\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(1)\n",
    "        elif rel not in preds: #FN\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(0)\n",
    "    for rel in preds: #FP\n",
    "        if rel not in game:\n",
    "            gold_narrations.append(0)\n",
    "            pred_narrations.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6fe940",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(gold_narrations) == len(pred_narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40e0cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(gold_narrations, pred_narrations, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbdf6788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8075, 0.8187579214195184, 0.8130899937067338, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3064fa3",
   "metadata": {},
   "source": [
    "##### step 6: get F1 for second pass on all narrations\n",
    "<br> This includes intra-turn narrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7818ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format gold -- include intra turn narrations!!\n",
    "gold_narrs = []\n",
    "for game in test_gold:\n",
    "    narrs = []\n",
    "    rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        if rel['type'] == 'Narration':\n",
    "            # if rel['y'] - rel['x'] < 11: ### D == 10 or less\n",
    "            if rel['y'] - rel['x'] < 16: ### D == 15 or less\n",
    "                narrs.append((rel['x'], rel['y']))\n",
    "    gold_narrs.append(narrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4aef591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add back in the intra narrations to the predictions \n",
    "all_narr_predictions = [p for p in predictions]\n",
    "for i, game in enumerate(test_gold):\n",
    "    narrs = []\n",
    "    rels = game['predicted_relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        if rel['type'] == 'Narration':\n",
    "            if turn_dict[rel['x']] == turn_dict[rel['y']]:\n",
    "                # if rel['y'] - rel['x'] < 11: ### D == 10 or less\n",
    "                if rel['y'] - rel['x'] < 16: ### D == 15 or less\n",
    "                    # print('heres one')\n",
    "                    narrs.append((i, rel['x'], rel['y'], 1, 13))\n",
    "    all_narr_predictions[i].extend(narrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4296d5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(all_narr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c2e7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now rearrange\n",
    "gold_narrations = []\n",
    "pred_narrations = []\n",
    "\n",
    "for i, game in enumerate(gold_narrs):\n",
    "    #preds = predictions[i]\n",
    "    # preds = [(p[1], p[2]) for p in predictions[i] if p[2]-p[1] < 11] ### D == 10 or less\n",
    "    preds = [(p[1], p[2]) for p in predictions[i] if p[2]-p[1] < 16] ### D == 15 or less\n",
    "    for rel in game:\n",
    "        if rel in preds: #TP\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(1)\n",
    "        elif rel not in preds: #FN\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(0)\n",
    "    for rel in preds: #FP\n",
    "        if rel not in game:\n",
    "            gold_narrations.append(0)\n",
    "            pred_narrations.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11a67be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(gold_narrations) == len(pred_narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1561286",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(gold_narrations, pred_narrations, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3563d1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.793398533007335, 0.8092269326683291, 0.8012345679012346, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129cf084",
   "metadata": {},
   "source": [
    "##### step 6: get F1 add all new Narration scores to the rest of the relations and re-compute the attachment and F1 scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc55e15",
   "metadata": {},
   "source": [
    "combine predictions and new test pred, <br> \n",
    "**but make sure that any relations that have the same endpoints as a new narrative arc are removed!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96e2f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###add the rest of the rel types preds to the predictions\n",
    "all_rel_predictions = [p for p in all_narr_predictions]\n",
    "sidelined_rels = []\n",
    "for i, game in enumerate(test_gold):\n",
    "    narrs = []\n",
    "    rels = game['predicted_relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    # new_arcs = [(p[1], p[2]) for p in all_narr_predictions[i] if p[2]-p[1] < 11] ## D== 10 or less\n",
    "    new_arcs = [(p[1], p[2]) for p in all_narr_predictions[i] if p[2]-p[1] < 16] ## D== 15 or less\n",
    "    for rel in rels:\n",
    "        if rel['type'] != 'Narration':\n",
    "            # if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 11:  ## D== 10 or less\n",
    "            if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 16:  ## D== 15 or less\n",
    "                ##IF ENDPOINTS ALREADY EXIST IN NEW NARRATIVE ARCS, DON'T COUNT THE REL\n",
    "                if (rel['x'], rel['y']) in new_arcs:\n",
    "                    sidelined_rels.append((i, rel['x'], rel['y'], 1, map_relations[rel['type']]))\n",
    "                else:\n",
    "                    narrs.append((i, rel['x'], rel['y'], 1, map_relations[rel['type']]))\n",
    "    all_rel_predictions[i].extend(narrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95add39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format gold --specify relation distance\n",
    "all_gold = []\n",
    "for i, game in enumerate(test_gold):\n",
    "    narrs = []\n",
    "    rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        # if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 11: ###D == 10 or less\n",
    "        if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 16: ###D == 15 or less\n",
    "            narrs.append((i, rel['x'], rel['y'], 1, map_relations[rel['type']]))\n",
    "    all_gold.append(narrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "068ca6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_gold), len(all_rel_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f40ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_TP = []\n",
    "matrix_list = []\n",
    "for i, game in enumerate(all_gold):\n",
    "    game_rels = [[p[1], p[2], p[4]] for p in game] #the distance here is already set in cell above\n",
    "    \n",
    "    # pred_rels = [[p[1], p[2], p[4]] for p in all_rel_predictions[i] if p[2] - p[1] < 11] ## D == 10 or less\n",
    "    pred_rels = [[p[1], p[2], p[4]] for p in all_rel_predictions[i] if p[2] - p[1] < 16] ## D == 15 or less\n",
    "\n",
    "    #true positives\n",
    "    #create the relation comparisons by type\n",
    "    TP = [e for e in pred_rels if e in game_rels] \n",
    "    leftover_pred = [p for p in pred_rels if p not in TP]\n",
    "    leftover_gold = [p for p in game_rels if p not in TP]\n",
    "\n",
    "    #then process the TP, FP, FN for matrix \n",
    "    total_TP.extend(TP)\n",
    "    #mlen = len(matrix_list)\n",
    "    rem_dict = defaultdict(list)\n",
    "    for x in TP:\n",
    "        matrix_list.append([x[2], x[2]])\n",
    "        # tp_matrix_list.append([x[0], x[0]])\n",
    "        #add to distance dict\n",
    "        # d = x[2]-x[1]\n",
    "        # tp_distances[d].append(x[0])\n",
    "    for x in leftover_pred:\n",
    "        rem_dict[(x[0], x[1])].append(('p', x[2]))\n",
    "    for x in leftover_gold:\n",
    "        rem_dict[(x[0], x[1])].append(('g', x[2]))\n",
    "\n",
    "    p_count = 0\n",
    "    g_count = 0\n",
    "    null_count = 0\n",
    "    for k in rem_dict.keys():\n",
    "        p = 16\n",
    "        t = 16\n",
    "        for re in rem_dict[k]:\n",
    "            if re[0] == 'p':\n",
    "                p = re[1]\n",
    "                p_count += 1\n",
    "            elif re[0] == 'g':\n",
    "                t = re[1]\n",
    "                g_count += 1\n",
    "        matrix_list.append([t,p])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaca2d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7337"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70030adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "gold = [m[0] for m in matrix_list]\n",
    "pred = [m[1] for m in matrix_list]\n",
    "# print(set(gold))\n",
    "# print(set(pred))\n",
    "all_gold_labels = []\n",
    "all_gold_labels.extend(gold)\n",
    "all_gold_labels.extend(pred)\n",
    "labels = list(set(all_gold_labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e41979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [reverse_relations[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "930647a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               Comment       0.62      0.57      0.59       323\n",
      "              Contrast       0.83      0.71      0.76        75\n",
      "            Correction       0.73      0.69      0.71       406\n",
      "  Question-answer_pair       0.88      0.86      0.87       409\n",
      "       Acknowledgement       0.88      0.84      0.86       895\n",
      "           Elaboration       0.78      0.74      0.76       806\n",
      "Clarification_question       0.77      0.82      0.79       190\n",
      "           Conditional       0.00      0.00      0.00         9\n",
      "          Continuation       0.44      0.59      0.50       324\n",
      "                Result       0.89      0.89      0.89      2003\n",
      "           Explanation       0.00      0.00      0.00        55\n",
      "                Q-Elab       0.53      0.37      0.43        49\n",
      "           Alternation       0.77      0.75      0.76        36\n",
      "             Narration       0.79      0.81      0.80       800\n",
      " Confirmation_question       0.95      0.93      0.94       222\n",
      "              Sequence       1.00      0.25      0.40         8\n",
      "                  Null       0.00      0.00      0.00       727\n",
      "\n",
      "              accuracy                           0.72      7337\n",
      "             macro avg       0.64      0.58      0.59      7337\n",
      "          weighted avg       0.72      0.72      0.72      7337\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold,pred,target_names=new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6be61c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Precision: 0.802879210508278\n",
      "Weighted Average Recall: 0.7950075642965204\n",
      "Weighted Average F1 score: 0.7976879694976644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "d = classification_report(gold,pred,target_names=new_labels,output_dict=True)\n",
    "prec = 0\n",
    "rec = 0\n",
    "f1 = 0 \n",
    "count = 0\n",
    "\n",
    "for label in new_labels:\n",
    "    if label!=\"Null\":\n",
    "        prec+=d[label][\"precision\"]*d[label][\"support\"]\n",
    "        rec+=d[label][\"recall\"]*d[label][\"support\"]\n",
    "        f1+=d[label][\"f1-score\"]*d[label][\"support\"]\n",
    "        count+=d[label][\"support\"]\n",
    "        # checking that support is same as the number of ground truth instance for the label\n",
    "        # assert d[label][\"support\"] == Counter(g_label_l)[label]\n",
    "        \n",
    "\n",
    "print(\"Weighted Average Precision:\", prec/count)\n",
    "print(\"Weighted Average Recall:\", rec/count)\n",
    "print(\"Weighted Average F1 score:\", f1/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c210aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attachment scores \n",
    "attach_pred = [1 if p!=16 else 0 for p in pred]\n",
    "attach_gold = [1 if g!=16 else 0 for g in gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48fada37",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(attach_gold, attach_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bc4b6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8885994483604045, 0.8773071104387292, 0.8829171741778319, None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd9738",
   "metadata": {},
   "source": [
    "##### STEP 6. Calculate the scores on from regular Llamipa outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92fed263",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_TP = []\n",
    "matrix_list = []\n",
    "for i, game in enumerate(test_gold):\n",
    "    game_rels = [] \n",
    "    rels = game['relations']\n",
    "    for rel in rels:\n",
    "        # if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 11: ##D == 10 or less\n",
    "        if rel['y'] > rel['x'] and rel['y'] - rel['x'] < 16: ##D == 15 or less\n",
    "            game_rels.append((i, rel['x'], rel['y'], 1, map_relations[rel['type']]))\n",
    "    \n",
    "    pred_rels = []\n",
    "    prels = game['predicted_relations']\n",
    "    for prel in prels:\n",
    "        # if prel['y'] > prel['x'] and prel['y'] - prel['x'] < 11: ##D == 10 or less\n",
    "        if prel['y'] > prel['x'] and prel['y'] - prel['x'] < 16: ##D == 15 or less\n",
    "            pred_rels.append((i, prel['x'], prel['y'], 1, map_relations[prel['type']]))\n",
    " \n",
    "    #true positives\n",
    "    #create the relation comparisons by type\n",
    "    TP = [e for e in pred_rels if e in game_rels] \n",
    "    leftover_pred = [p for p in pred_rels if p not in TP]\n",
    "    leftover_gold = [p for p in game_rels if p not in TP]\n",
    "\n",
    "    #then process the TP, FP, FN for matrix \n",
    "    total_TP.extend(TP)\n",
    "    #mlen = len(matrix_list)\n",
    "    rem_dict = defaultdict(list)\n",
    "    for x in TP:\n",
    "        matrix_list.append([x[4], x[4]])\n",
    "        # tp_matrix_list.append([x[0], x[0]])\n",
    "        #add to distance dict\n",
    "        # d = x[2]-x[1]\n",
    "        # tp_distances[d].append(x[0])\n",
    "    for x in leftover_pred:\n",
    "        rem_dict[(x[1], x[2])].append(('p', x[4]))\n",
    "    for x in leftover_gold:\n",
    "        rem_dict[(x[1], x[2])].append(('g', x[4]))\n",
    "\n",
    "    p_count = 0\n",
    "    g_count = 0\n",
    "    null_count = 0\n",
    "    for k in rem_dict.keys():\n",
    "        p = 16\n",
    "        t = 16\n",
    "        for re in rem_dict[k]:\n",
    "            if re[0] == 'p':\n",
    "                p = re[1]\n",
    "                p_count += 1\n",
    "            elif re[0] == 'g':\n",
    "                t = re[1]\n",
    "                g_count += 1\n",
    "        matrix_list.append([t,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90e54c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "gold = [m[0] for m in matrix_list]\n",
    "pred = [m[1] for m in matrix_list]\n",
    "# print(set(gold))\n",
    "# print(set(pred))\n",
    "all_gold_labels = []\n",
    "all_gold_labels.extend(gold)\n",
    "all_gold_labels.extend(pred)\n",
    "labels = list(set(all_gold_labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3dade6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [reverse_relations[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d057ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               Comment       0.62      0.57      0.59       323\n",
      "              Contrast       0.83      0.71      0.76        75\n",
      "            Correction       0.73      0.69      0.71       406\n",
      "  Question-answer_pair       0.88      0.86      0.87       409\n",
      "       Acknowledgement       0.88      0.84      0.86       895\n",
      "           Elaboration       0.78      0.74      0.76       806\n",
      "Clarification_question       0.77      0.82      0.79       190\n",
      "           Conditional       0.00      0.00      0.00         9\n",
      "          Continuation       0.44      0.59      0.50       324\n",
      "                Result       0.89      0.89      0.89      2003\n",
      "           Explanation       0.00      0.00      0.00        55\n",
      "                Q-Elab       0.53      0.37      0.43        49\n",
      "           Alternation       0.77      0.75      0.76        36\n",
      "             Narration       0.80      0.76      0.78       800\n",
      " Confirmation_question       0.95      0.93      0.94       222\n",
      "              Sequence       1.00      0.25      0.40         8\n",
      "                  Null       0.00      0.00      0.00       711\n",
      "\n",
      "              accuracy                           0.71      7321\n",
      "             macro avg       0.64      0.57      0.59      7321\n",
      "          weighted avg       0.73      0.71      0.72      7321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold,pred,target_names=new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15ea949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Average Precision: 0.8034073733878314\n",
      "Weighted Average Recall: 0.7881996974281392\n",
      "Weighted Average F1 score: 0.7944759859918737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kate/miniconda3/envs/bert_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "d = classification_report(gold,pred,target_names=new_labels,output_dict=True)\n",
    "prec = 0\n",
    "rec = 0\n",
    "f1 = 0 \n",
    "count = 0\n",
    "\n",
    "for label in new_labels:\n",
    "    if label!=\"Null\":\n",
    "        prec+=d[label][\"precision\"]*d[label][\"support\"]\n",
    "        rec+=d[label][\"recall\"]*d[label][\"support\"]\n",
    "        f1+=d[label][\"f1-score\"]*d[label][\"support\"]\n",
    "        count+=d[label][\"support\"]\n",
    "        # checking that support is same as the number of ground truth instance for the label\n",
    "        # assert d[label][\"support\"] == Counter(g_label_l)[label]\n",
    "        \n",
    "\n",
    "print(\"Weighted Average Precision:\", prec/count)\n",
    "print(\"Weighted Average Recall:\", rec/count)\n",
    "print(\"Weighted Average F1 score:\", f1/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d93dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attachment scores \n",
    "attach_pred = [1 if p!=16 else 0 for p in pred]\n",
    "attach_gold = [1 if g!=16 else 0 for g in gold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1112bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(attach_gold, attach_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "163cf4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8900232018561485, 0.8704992435703479, 0.880152963671128, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8ae98",
   "metadata": {},
   "source": [
    "##### STEP 7. Caluclate Llamipa score on just Narrative arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdfb04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_arcs = []\n",
    "pred_arcs = []\n",
    "for game in test_gold:\n",
    "    narrs = []\n",
    "    rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        if rel['type'] == 'Narration':\n",
    "            if turn_dict[rel['x']] != turn_dict[rel['y']]:\n",
    "                # if rel['y'] - rel['x'] < 11: ## D == 10 or less\n",
    "                if rel['y'] - rel['x'] < 16: ## D == 15 or less\n",
    "                    narrs.append((rel['x'], rel['y']))\n",
    "    gold_arcs.append(narrs)\n",
    "    narrs = []\n",
    "    prels = game['predicted_relations']\n",
    "    for prel in prels:\n",
    "        if prel['type'] == 'Narration':\n",
    "            if turn_dict[prel['x']] != turn_dict[prel['y']]:\n",
    "                # if prel['y'] - prel['x'] < 11:  ## D == 10 or less\n",
    "                if prel['y'] - prel['x'] < 16:  ## D == 15 or less\n",
    "                    narrs.append((prel['x'], prel['y']))\n",
    "    pred_arcs.append(narrs)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc1dce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now rearrange\n",
    "gold_narrations = []\n",
    "pred_narrations = []\n",
    "\n",
    "for i, game in enumerate(gold_arcs):\n",
    "    preds = pred_arcs[i]\n",
    "    for rel in game:\n",
    "        if rel in preds: #TP\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(1)\n",
    "        elif rel not in preds: #FN\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(0)\n",
    "    for rel in preds: #FP\n",
    "        if rel not in game:\n",
    "            gold_narrations.append(0)\n",
    "            pred_narrations.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbaff383",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(gold_narrations) == len(pred_narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02450a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(gold_narrations, pred_narrations, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7b98c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179347826086957, 0.7629911280101395, 0.7895081967213115, None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc0251",
   "metadata": {},
   "source": [
    "#### So what is the second pass score on all narrative arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8c79f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home + '/pickles/' + 'llamipa_second_pass.pkl', 'rb') as f:\n",
    "    secondpass = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe99be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_arcs = []\n",
    "for game in test_gold:\n",
    "    narrs = []\n",
    "    rels = game['relations']\n",
    "    edus = game['edus']\n",
    "    turn_dict = {edu['global_index']:edu['turn'] for edu in edus}\n",
    "    for rel in rels:\n",
    "        if rel['type'] == 'Narration':\n",
    "            if turn_dict[rel['x']] != turn_dict[rel['y']]:\n",
    "                # if rel['y'] - rel['x'] < 11: ## D == 10 or less\n",
    "                # if rel['y'] - rel['x'] < 16: ## D == 15 or less\n",
    "                #NO distance limit\n",
    "                    narrs.append((rel['x'], rel['y']))\n",
    "    gold_arcs.append(narrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fe4bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_narrations = []\n",
    "pred_narrations = []\n",
    "secondpass_falsepos = []\n",
    "secondpass_falseneg = []\n",
    "\n",
    "for i, game in enumerate(gold_arcs):\n",
    "    #preds = predictions[i]\n",
    "    # preds = [(p[1], p[2]) for p in predictions[i] if p[2]-p[1] < 11] ### D == 10 or less\n",
    "    preds = [(p[1], p[2]) for p in secondpass[i]] ## No distance\n",
    "    for rel in game:\n",
    "        if rel in preds: #TP\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(1)\n",
    "        elif rel not in preds: #FN\n",
    "            gold_narrations.append(1)\n",
    "            pred_narrations.append(0)\n",
    "            secondpass_falseneg.append([i, rel[0], rel[1]])\n",
    "    for rel in preds: #FP\n",
    "        if rel not in game:\n",
    "            gold_narrations.append(0)\n",
    "            pred_narrations.append(1)\n",
    "            secondpass_falsepos.append([i, rel[0], rel[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9cadb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(gold_narrations) == len(pred_narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0b74947",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = precision_recall_fscore_support(gold_narrations, pred_narrations, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa387417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7942238267148014, 0.8088235294117647, 0.8014571948998178, None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2f019",
   "metadata": {},
   "source": [
    "#### comparison of Narrations in Gold and 2p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9010923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 29,\n",
       "         3: 26,\n",
       "         2: 22,\n",
       "         8: 13,\n",
       "         5: 12,\n",
       "         6: 11,\n",
       "         11: 8,\n",
       "         9: 8,\n",
       "         15: 5,\n",
       "         12: 5,\n",
       "         7: 5,\n",
       "         10: 5,\n",
       "         18: 4,\n",
       "         13: 4,\n",
       "         16: 3,\n",
       "         19: 2,\n",
       "         17: 2,\n",
       "         22: 2,\n",
       "         39: 2,\n",
       "         14: 1,\n",
       "         44: 1,\n",
       "         21: 1})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([s[2] - s[1] for s in secondpass_falsepos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c14d0c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 19,\n",
       "         6: 19,\n",
       "         7: 16,\n",
       "         4: 15,\n",
       "         8: 12,\n",
       "         5: 12,\n",
       "         2: 11,\n",
       "         9: 10,\n",
       "         10: 8,\n",
       "         13: 7,\n",
       "         14: 5,\n",
       "         11: 5,\n",
       "         15: 3,\n",
       "         20: 3,\n",
       "         17: 2,\n",
       "         16: 2,\n",
       "         18: 2,\n",
       "         24: 1,\n",
       "         21: 1,\n",
       "         12: 1,\n",
       "         27: 1,\n",
       "         29: 1})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([s[2] - s[1] for s in secondpass_falseneg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "de00c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 171)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(secondpass_falseneg), len(secondpass_falsepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "547e5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(home + \"/secondpass_llamipa_falsenegative.txt\",\"w\")\n",
    "for i, game in enumerate(test_gold):\n",
    "    fp = [f for f in secondpass_falseneg if f[0] == i]\n",
    "    if len(fp) > 0:\n",
    "        edus = game['edus']\n",
    "        gameid = game['id']\n",
    "        # print(gameid)\n",
    "        # print(fp)\n",
    "        print(gameid, file=f)\n",
    "        for rel in fp:\n",
    "            print(str(rel[1]) + '. ' + edus[rel[1]]['speaker'] + ': ' + edus[rel[1]]['text'], file=f)\n",
    "            print('------->>', file=f)\n",
    "            print(str(rel[2]) + '. ' + edus[rel[2]]['speaker'] + ': ' + edus[rel[2]]['text'], file=f)\n",
    "            print('===', file=f)\n",
    "        print('------------------------------\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06687f",
   "metadata": {},
   "source": [
    "#### So what exactly is the difference between llamipa's arcs and the narrative pass arcs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d87af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_arcs), len(secondpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondpass[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bea061",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_in_llamipa = []\n",
    "just_in_pass = []\n",
    "for i, x in enumerate(pred_arcs):\n",
    "    # print('llamipa only: ', x)\n",
    "    check = [(c[1], c[2]) for c in secondpass[i] if c[2] - c[1] < 11]\n",
    "    # print('second pass:', check)\n",
    "    # print('-----------------------------')\n",
    "    for rel in check:\n",
    "        if rel not in x:\n",
    "            just_in_pass.append((i, rel[0], rel[1]))\n",
    "    for z in x:\n",
    "        if z not in check:\n",
    "            just_in_llamipa.append((i, z[0], z[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f271d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_in_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_in_llamipa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a87a9",
   "metadata": {},
   "source": [
    "### now figure out which are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for rel in just llamipa: \n",
    "#see if it's in the gold\n",
    "#if not, see if \n",
    "big_list = []\n",
    "table_lables = ['index', 'llamipa only', 'llamipa_correct', 'pass only', 'pass_correct']\n",
    "for i, game in enumerate(gold_arcs):\n",
    "    #get the just llamipas\n",
    "    g = [] # game index, #jl, #jl correct, #sp, #spcorrect\n",
    "    g.append(i)\n",
    "    jl = [(m[1], m[2]) for m in just_in_llamipa if m[0] == i]\n",
    "    if len(jl) > 0:\n",
    "        #g.append(len(jl))\n",
    "        g.append(jl)  \n",
    "        ojl = [] #overlap between jl and gold\n",
    "        for n in jl:\n",
    "            if n in game:\n",
    "                ojl.append(n)\n",
    "        if len(ojl) > 0:\n",
    "            g.append(len(ojl))\n",
    "        else:\n",
    "            g.append('None')\n",
    "    else:\n",
    "        g.extend(['None', 'None'])\n",
    "\n",
    "    sp = [(m[1], m[2]) for m in just_in_pass if m[0] == i]\n",
    "    if len(sp) > 0:\n",
    "        # g.append(len(sp))\n",
    "        g.append(sp)\n",
    "        osp = []\n",
    "        for o in sp:\n",
    "            if o in game:\n",
    "                osp.append(o)\n",
    "        if len(osp) > 0:\n",
    "            g.append(len(osp))\n",
    "        else:\n",
    "            g.append('None')\n",
    "    else:\n",
    "        g.extend(['None', 'None'])\n",
    "    big_list.append(g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add656ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list_edit = [b for b in big_list if b[1]!='None' or b[3]!='None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('                                         ')\n",
    "print(pandas.DataFrame(big_list_edit, columns=table_lables))\n",
    "print('                                         ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8627533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_arcs  = \n",
    "sum([len(l) for l in gold_arcs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
